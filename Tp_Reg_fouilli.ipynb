{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC - TP - Prediction by regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>15.6</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>-0.6946</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17.7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>15.3</td>\n",
       "      <td>17.6</td>\n",
       "      <td>19.5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>1.8794</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114</td>\n",
       "      <td>16.2</td>\n",
       "      <td>19.7</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>-0.1736</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94</td>\n",
       "      <td>17.4</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-2.9544</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>84</td>\n",
       "      <td>13.3</td>\n",
       "      <td>17.7</td>\n",
       "      <td>17.8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>-1.2856</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>77</td>\n",
       "      <td>16.2</td>\n",
       "      <td>20.8</td>\n",
       "      <td>22.1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.6946</td>\n",
       "      <td>-2.0000</td>\n",
       "      <td>-1.3681</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>16.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>0.8682</td>\n",
       "      <td>0.8682</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>83</td>\n",
       "      <td>16.9</td>\n",
       "      <td>19.8</td>\n",
       "      <td>22.1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>-3.7588</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>70</td>\n",
       "      <td>15.7</td>\n",
       "      <td>18.6</td>\n",
       "      <td>20.7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.0419</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10\n",
       "0     87  15.6  18.5  18.4   4   4   8  0.6946 -1.7101 -0.6946   84\n",
       "1     82  17.0  18.4  17.7   5   5   7 -4.3301 -4.0000 -3.0000   87\n",
       "2     92  15.3  17.6  19.5   2   5   4  2.9544  1.8794  0.5209   82\n",
       "3    114  16.2  19.7  22.5   1   1   0  0.9848  0.3473 -0.1736   92\n",
       "4     94  17.4  20.5  20.4   8   8   7 -0.5000 -2.9544 -4.3301  114\n",
       "..   ...   ...   ...   ...  ..  ..  ..     ...     ...     ...  ...\n",
       "96    84  13.3  17.7  17.8   3   5   6  0.0000 -1.0000 -1.2856   76\n",
       "97    77  16.2  20.8  22.1   6   5   5 -0.6946 -2.0000 -1.3681   71\n",
       "98    99  16.9  23.0  22.6   6   4   7  1.5000  0.8682  0.8682   77\n",
       "99    83  16.9  19.8  22.1   6   5   3 -4.0000 -3.7588 -4.0000   99\n",
       "100   70  15.7  18.6  20.7   7   7   7  0.0000 -1.0419 -4.0000   83\n",
       "\n",
       "[101 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "ozone = pd.read_csv('ozone.txt', sep = ' ')\n",
    "ozone\n",
    "# y is the target variable, the other are predictive variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : How many individuals are there in this dataset ? How many variables (including the target one) describe them ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ozone.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "101 individuals and 11 variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This dataset represents some meteorological information taken at different days during different periods.\n",
    " - y represents the maximal ozone (O3) concentration in the air during a given day\n",
    " - x1, x2, x3 are the temperatures at 9am, 12am, and 3pm during the same day\n",
    " - x4, x5, x6 represent the cloudiness at 9am, 12am, and 3pm during the same day\n",
    " - x7, x8, x9 represent the wind projected on the axis East-West at 9am, 12am, and 3pm during the same day\n",
    " - x10 is the maximal ozone (O3) concentration in the air during the day before.\n",
    " \n",
    "In this TP, we aim at predicting y using one or more of the available predictive variables (x1 to x10).\n",
    "We will apply the different techniques seen during the course to evaluate different regression models and try to choose the most adapted one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : Simple linear regression to predict y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this exercice, we will assume that we can only use one variable to predict y. Our objective is to find which variable is the best one for that.\n",
    "We will start by considering x1 to predict y."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Draw a plot to visualize the relationship between x1 (x-axis) and y (y-axis)\n",
    "Add a title, and labels for the axes.\n",
    "Hint : We want to visualize the points in a 2D graph (no line between the points).\n",
    "plt.scatter makes the job, or plt.plot but with 'o' as parameter to indicate that we want only the points (no lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Ozone concentration')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "#data_x1 = pd.DataFrame(ozone, columns=['x1'])\n",
    "#data_y = pd.DataFrame(ozone, columns=['y'])\n",
    "\n",
    "data_x1 = ozone['x1']\n",
    "data_y = ozone['y']\n",
    "\n",
    "plt.scatter(data_x1,data_y, marker = 'o', c = 'green' )\n",
    "\n",
    "#plt.scatter(data_x1, marker = 'o', color = 'green', label = 'Linear regression')\n",
    "\n",
    "plt.title(\"Simple linear regression\") # gives a title to the figure\n",
    "plt.xlabel(\"Temperature\") # gives a label to the x-axis\n",
    "plt.ylabel(\"Ozone concentration\")# gives a label to the y-axis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Do you think that a regression model can help here to predict y using x1 ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Oui, la repartition des points peut etre approximée par une droite"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will now create a regression model with y as target variable and x1 as predictive variable.\n",
    "For that, there are different possibilities in Python. We will use here the OLS function of the statsmodel package.\n",
    "It works like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     const    x1\n",
      "0      1.0  15.6\n",
      "1      1.0  17.0\n",
      "2      1.0  15.3\n",
      "3      1.0  16.2\n",
      "4      1.0  17.4\n",
      "..     ...   ...\n",
      "96     1.0  13.3\n",
      "97     1.0  16.2\n",
      "98     1.0  16.9\n",
      "99     1.0  16.9\n",
      "100    1.0  15.7\n",
      "\n",
      "[101 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   105.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 12 Apr 2021</td> <th>  Prob (F-statistic):</th> <td>2.78e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:38:24</td>     <th>  Log-Likelihood:    </th> <td> -444.69</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   101</td>      <th>  AIC:               </th> <td>   893.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    99</td>      <th>  BIC:               </th> <td>   898.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  -33.0106</td> <td>   12.221</td> <td>   -2.701</td> <td> 0.008</td> <td>  -57.259</td> <td>   -8.762</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    6.7460</td> <td>    0.657</td> <td>   10.273</td> <td> 0.000</td> <td>    5.443</td> <td>    8.049</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.779</td> <th>  Durbin-Watson:     </th> <td>   0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.249</td> <th>  Jarque-Bera (JB):  </th> <td>   2.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.360</td> <th>  Prob(JB):          </th> <td>   0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.163</td> <th>  Cond. No.          </th> <td>    115.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.516\n",
       "Model:                            OLS   Adj. R-squared:                  0.511\n",
       "Method:                 Least Squares   F-statistic:                     105.5\n",
       "Date:                Mon, 12 Apr 2021   Prob (F-statistic):           2.78e-17\n",
       "Time:                        17:38:24   Log-Likelihood:                -444.69\n",
       "No. Observations:                 101   AIC:                             893.4\n",
       "Df Residuals:                      99   BIC:                             898.6\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -33.0106     12.221     -2.701      0.008     -57.259      -8.762\n",
       "x1             6.7460      0.657     10.273      0.000       5.443       8.049\n",
       "==============================================================================\n",
       "Omnibus:                        2.779   Durbin-Watson:                   0.900\n",
       "Prob(Omnibus):                  0.249   Jarque-Bera (JB):                2.289\n",
       "Skew:                           0.360   Prob(JB):                        0.318\n",
       "Kurtosis:                       3.163   Cond. No.                         115.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "X = ozone['x1'] # Select the column containing the predictive variable\n",
    "X = sm.add_constant(X) # add a constant column for the constant term of the model (for the parameter beta_0)\n",
    "print(X) # you should see 2 columns in X : a constant one (const) with ones everywhere and another with the values of x1\n",
    "Y = ozone['y'] # Select the target variable and store it in Y\n",
    "model = sm.OLS(Y, X).fit() # fit the model to predict Y using X\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can see above a lot of informations about the created model. The things that we need are : \n",
    "    - the coefficients of the model : in the column 'coef'. 2 coefficients here (beta0 for const and beta1 for x1). You can access them by model.params\n",
    "    - the uncertainty about the estimation of the coefficient : column 'std err'. We are only interested in the one for beta_1. You can access them by model.bse\n",
    "    - the value of the student's test about x1 : in the column 't'. You can access it by model.tvalues\n",
    "    - the critical probability of the student's test about x1 : column 'P>|t|'. You can access it by model.pvalues"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Questions : \n",
    " - What is the equation of the model ?\n",
    " - What can you conclude about the influence of x1 on y ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y=-33.0106 + 6.7460*x1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "P>|t| = 0.000 << 0.05 donc x1 influence bien y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Questions: \n",
    " - What should be the prediction made by the model for the first individual of the ozone dataset ? (index 0)\n",
    " - What is the residual (error) for this individual ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.772999999999996"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=-33.0106 + 6.7460*ozone['x1'][0]\n",
    "y\n",
    "\n",
    "r = (ozone['y'][0]-y)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      72.226485\n",
       "1      81.670840\n",
       "2      70.202695\n",
       "3      76.274066\n",
       "4      84.369227\n",
       "         ...    \n",
       "96     56.710759\n",
       "97     76.274066\n",
       "98     80.996243\n",
       "99     80.996243\n",
       "100    72.901082\n",
       "Length: 101, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the predictions made by the model on the dataset used to fit the model can be accessed by\n",
    "model.fittedvalues\n",
    "# Check that the fitted value for the first individual is equal to the one you computed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      14.773515\n",
       "1       0.329160\n",
       "2      21.797305\n",
       "3      37.725934\n",
       "4       9.630773\n",
       "         ...    \n",
       "96     27.289241\n",
       "97      0.725934\n",
       "98     18.003757\n",
       "99      2.003757\n",
       "100    -2.901082\n",
       "Length: 101, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the residuals made by the model on the dataset used to fit the model can be accessed by\n",
    "model.resid\n",
    "# Check that the residual for the first individual is equal to the one you computed above"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Plot on the same graph the data (y VS x1, as before) and the regression model (the fitted values define the regression line). Add labels to the axis and a legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Ozone concentration')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "#data_x1 = pd.DataFrame(ozone, columns=['x1'])\n",
    "#data_y = pd.DataFrame(ozone, columns=['y'])\n",
    "\n",
    "data_x1 = ozone['x1']\n",
    "data_y = ozone['y']\n",
    "data_reg = model.fittedvalues\n",
    "\n",
    "plt.scatter(data_x1,data_y, marker = 'o', c = 'green' )\n",
    "plt.plot(data_x1,data_reg, c = 'blue' )\n",
    "\n",
    "#plt.scatter(data_x1, marker = 'o', color = 'green', label = 'Linear regression')\n",
    "\n",
    "plt.title(\"Simple linear regression\") # gives a title to the figure\n",
    "plt.xlabel(\"Temperature\") # gives a label to the x-axis\n",
    "plt.ylabel(\"Ozone concentration\")# gives a label to the y-axis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We have seen in the CM two important quantities related to a regression model : the coefficient of determination (R^2) and I_r, the sum of squared residuals (residuals are errors made by the model on the data used to create it).\n",
    "The R^2 can be obtained by model.rsquared \n",
    "The residuals can be obtained by model.resid as explained before"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Questions : \n",
    " - What is the R^2 of this model ?\n",
    " - Compute I_r\n",
    " - Compute I_t according to the formula in the slides. (you can use np.sum() and np.mean())\n",
    " - Deduce I_m \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5159750701422355"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_carre = model.rsquared\n",
    "r_carre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39455.804936241766"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_r = np.sum((ozone['y'] - model.fittedvalues)**2)\n",
    "I_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81516.05940594061"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_moy = np.mean(ozone['y'])\n",
    "I_t = np.sum((ozone['y'] - y_moy)**2)\n",
    "I_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42060.25446969885"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_m = I_t - I_r\n",
    "I_m"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To compare models with one predictive variable, we can use :\n",
    " - R^2 (higher is better)\n",
    " - I_r (lower is better)\n",
    " - the critical probability of the student's test on the predictive variable (lower is better)\n",
    "We have seen how to get these 3 quantities.\n",
    "We will now try another simple regression model (with another predictive variable) and compare these quantities."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Compute these quantities for a simple regression model that predicts y using x4. Which variable (x1 or x4) seems more adpated to predict y ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     const  x4\n",
      "0      1.0   4\n",
      "1      1.0   5\n",
      "2      1.0   2\n",
      "3      1.0   1\n",
      "4      1.0   8\n",
      "..     ...  ..\n",
      "96     1.0   3\n",
      "97     1.0   6\n",
      "98     1.0   6\n",
      "99     1.0   6\n",
      "100    1.0   7\n",
      "\n",
      "[101 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.378</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   60.21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 12 Apr 2021</td> <th>  Prob (F-statistic):</th> <td>7.83e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:55:57</td>     <th>  Log-Likelihood:    </th> <td> -457.34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   101</td>      <th>  AIC:               </th> <td>   918.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    99</td>      <th>  BIC:               </th> <td>   923.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  124.1098</td> <td>    4.840</td> <td>   25.640</td> <td> 0.000</td> <td>  114.505</td> <td>  133.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -6.6894</td> <td>    0.862</td> <td>   -7.759</td> <td> 0.000</td> <td>   -8.400</td> <td>   -4.979</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 8.975</td> <th>  Durbin-Watson:     </th> <td>   0.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.011</td> <th>  Jarque-Bera (JB):  </th> <td>   9.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.749</td> <th>  Prob(JB):          </th> <td> 0.00891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.999</td> <th>  Cond. No.          </th> <td>    12.4</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.378\n",
       "Model:                            OLS   Adj. R-squared:                  0.372\n",
       "Method:                 Least Squares   F-statistic:                     60.21\n",
       "Date:                Mon, 12 Apr 2021   Prob (F-statistic):           7.83e-12\n",
       "Time:                        17:55:57   Log-Likelihood:                -457.34\n",
       "No. Observations:                 101   AIC:                             918.7\n",
       "Df Residuals:                      99   BIC:                             923.9\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        124.1098      4.840     25.640      0.000     114.505     133.714\n",
       "x4            -6.6894      0.862     -7.759      0.000      -8.400      -4.979\n",
       "==============================================================================\n",
       "Omnibus:                        8.975   Durbin-Watson:                   0.921\n",
       "Prob(Omnibus):                  0.011   Jarque-Bera (JB):                9.441\n",
       "Skew:                           0.749   Prob(JB):                      0.00891\n",
       "Kurtosis:                       2.999   Cond. No.                         12.4\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "X = ozone['x4'] # Select the column containing the predictive variable\n",
    "X = sm.add_constant(X) # add a constant column for the constant term of the model (for the parameter beta_0)\n",
    "print(X) # you should see 2 columns in X : a constant one (const) with ones everywhere and another with the values of x1\n",
    "Y = ozone['y'] # Select the target variable and store it in Y\n",
    "model = sm.OLS(Y, X).fit() # fit the model to predict Y using X\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37817872554611"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_carre = model.rsquared\n",
    "r_carre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50688.419948261"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_r = np.sum((ozone['y'] - model.fittedvalues)**2)\n",
    "I_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81516.05940594061"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_moy = np.mean(ozone['y'])\n",
    "I_t = np.sum((ozone['y'] - y_moy)**2)\n",
    "I_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30827.639457679616"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_m = I_t - I_r\n",
    "I_m"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Select, according to these 3 criteria, the best variable (amongst the 10 predictive ones) to predict y.\n",
    "Hint : you can use a 'for' loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 : 0.5159750701422355\n",
      "x2 : 0.6128945664852581\n",
      "x3 : 0.5973574009499361\n",
      "x4 : 0.37817872554611\n",
      "x5 : 0.39692178579182213\n",
      "x6 : 0.22569404027313866\n",
      "x7 : 0.2600824051363315\n",
      "x8 : 0.18621446723998814\n",
      "x9 : 0.1443666993650864\n",
      "x10 : 0.4697289163946735\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "for x in ozone.columns[1:]:\n",
    "    X = ozone[x]\n",
    "    X = sm.add_constant(X) # add a constant column for the constant term of the model (for the parameter beta_0)\n",
    "    Y = ozone['y'] # Select the target variable and store it in Y\n",
    "    model = sm.OLS(Y, X).fit() # fit the model to predict Y using X\n",
    "    print(x+' : '+str(model.rsquared))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31555.309514750104"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "X = ozone['x2'] # Select the column containing the predictive variable\n",
    "X = sm.add_constant(X) # add a constant column for the constant term of the model (for the parameter beta_0) x1\n",
    "Y = ozone['y'] # Select the target variable and store it in Y\n",
    "model = sm.OLS(Y, X).fit() # fit the model to predict Y using X\n",
    "I_r = np.sum((ozone['y'] - model.fittedvalues)**2)\n",
    "I_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we will see how this best model behaves when predicting new data, and compare with the first model that we have tried (using x1). For this, we will 'cheat' (but we'll do it in a more proper way a bit later) : you will find\n",
    "10 new individuals in the file 'ozone_n.txt'. For these 10 new individuals, we have the values of the 10 predictive variables and also the value of the target variable. In a real prediction setting, you will never have new individuals with the ground truth (values of the target variable), but here we will do like that to start. We can consider that these new individuals are our test set. In a latter exercice, I will show you how to do a proper train / test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>18.3</td>\n",
       "      <td>21.9</td>\n",
       "      <td>22.9</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1.2856</td>\n",
       "      <td>-2.2981</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>13.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.2139</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>19.9</td>\n",
       "      <td>21.6</td>\n",
       "      <td>20.4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-4.5963</td>\n",
       "      <td>-5.1962</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>18.1</td>\n",
       "      <td>21.2</td>\n",
       "      <td>23.9</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.5981</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>-3.7588</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97</td>\n",
       "      <td>20.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.7101</td>\n",
       "      <td>-2.7362</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59</td>\n",
       "      <td>18.3</td>\n",
       "      <td>18.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70</td>\n",
       "      <td>17.1</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-7.8785</td>\n",
       "      <td>-5.1962</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>81</td>\n",
       "      <td>19.6</td>\n",
       "      <td>25.1</td>\n",
       "      <td>27.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>-2.5712</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>146</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>6.5778</td>\n",
       "      <td>4.3301</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78</td>\n",
       "      <td>17.7</td>\n",
       "      <td>20.2</td>\n",
       "      <td>21.5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10\n",
       "0  106  18.3  21.9  22.9   5   6   8  1.2856 -2.2981 -3.9392  101\n",
       "1   60  13.7  14.0  15.8   4   5   4  0.0000  3.2139  0.0000   71\n",
       "2   72  19.9  21.6  20.4   7   7   8 -3.0000 -4.5963 -5.1962   65\n",
       "3   72  18.1  21.2  23.9   7   6   4 -2.5981 -3.9392 -3.7588  113\n",
       "4   97  20.8  23.7  25.0   2   3   4  0.0000  1.7101 -2.7362   93\n",
       "5   59  18.3  18.3  19.0   7   7   7 -3.9392 -1.9284 -1.7101   66\n",
       "6   70  17.1  18.2  18.0   7   7   7 -4.3301 -7.8785 -5.1962   72\n",
       "7   81  19.6  25.1  27.2   3   4   4 -1.9284 -2.5712 -4.3301   57\n",
       "8  146  27.0  32.7  33.7   0   0   0  2.9544  6.5778  4.3301  121\n",
       "9   78  17.7  20.2  21.5   5   5   3  0.0000  0.5209  0.0000   59"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load these new individuals.\n",
    "ozone_new = pd.read_csv('ozone_n.txt', sep = ' ')\n",
    "ozone_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     90.440598\n",
       "1     59.409146\n",
       "2    101.234147\n",
       "3     89.091404\n",
       "4    107.305518\n",
       "5     90.440598\n",
       "6     82.345437\n",
       "7     99.210356\n",
       "8    149.130518\n",
       "9     86.393017\n",
       "dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I show you here how to predict the values of new data using a model\n",
    "# First, we will fit a model to predict y using x1 with the ozone data as before (we don't use the new individuals to fit the model)\n",
    "X = ozone.iloc[:,1] \n",
    "X = sm.add_constant(X) \n",
    "Y = ozone['y']\n",
    "model = sm.OLS(Y, X).fit() \n",
    "# Then, we will prepare the new data so that it is under the same form as the data used to create the model,\n",
    "# i.e. one constant column and one column with x1 values for the new data\n",
    "X_new = ozone_new.iloc[:,1] # only the x1 column of the new dataset\n",
    "X_new = sm.add_constant(X_new) # add the constant column\n",
    "# and then we can easily predict y for this new dataset:\n",
    "model.predict(X_new)\n",
    "# You can see the predictions for the 10 individuals of ozone_new"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : What is the mean squared error of these predictions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304.81801932071556"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_error = np.mean((ozone_new['y'] - model.predict(X_new))**2)\n",
    "mean_error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Do the same for the best model that you found above, and compare the mean squared errors of the 2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     92.928088\n",
       "1     48.599551\n",
       "2     91.244726\n",
       "3     89.000243\n",
       "4    103.028261\n",
       "5     72.727742\n",
       "6     72.166622\n",
       "7    110.883951\n",
       "8    153.529126\n",
       "9     83.389036\n",
       "dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I show you here how to predict the values of new data using a model\n",
    "# First, we will fit a model to predict y using x1 with the ozone data as before (we don't use the new individuals to fit the model)\n",
    "X = ozone['x2'] \n",
    "X = sm.add_constant(X) \n",
    "Y = ozone['y']\n",
    "model = sm.OLS(Y, X).fit() \n",
    "# Then, we will prepare the new data so that it is under the same form as the data used to create the model,\n",
    "# i.e. one constant column and one column with x1 values for the new data\n",
    "X_new = ozone_new.iloc[:,2] # only the x1 column of the new dataset\n",
    "X_new = sm.add_constant(X_new) # add the constant column\n",
    "# and then we can easily predict y for this new dataset:\n",
    "model.predict(X_new)\n",
    "# You can see the predictions for the 10 individuals of ozone_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216.84779280541434"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_error = np.mean((ozone_new['y'] - model.predict(X_new))**2)\n",
    "mean_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Multiple regression to predict y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will now apply multiple regression (more than one predictive variable) to predict y. To perform multiple regression, we can use the OLS function as before but we just need to put more variables in the X dataframe used as input to OLS.\n",
    "For instance, below we will create a model to predict y using x2 and x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   78.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 12 Apr 2021</td> <th>  Prob (F-statistic):</th> <td>4.62e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:00:23</td>     <th>  Log-Likelihood:    </th> <td> -433.07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   101</td>      <th>  AIC:               </th> <td>   872.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   880.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  -33.8288</td> <td>   10.950</td> <td>   -3.089</td> <td> 0.003</td> <td>  -55.558</td> <td>  -12.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.0269</td> <td>    1.279</td> <td>    0.803</td> <td> 0.424</td> <td>   -1.512</td> <td>    3.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    4.9153</td> <td>    0.976</td> <td>    5.034</td> <td> 0.000</td> <td>    2.978</td> <td>    6.853</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.676</td> <th>  Durbin-Watson:     </th> <td>   0.980</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.713</td> <th>  Jarque-Bera (JB):  </th> <td>   0.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.163</td> <th>  Prob(JB):          </th> <td>   0.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.708</td> <th>  Cond. No.          </th> <td>    177.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.615\n",
       "Model:                            OLS   Adj. R-squared:                  0.608\n",
       "Method:                 Least Squares   F-statistic:                     78.41\n",
       "Date:                Mon, 12 Apr 2021   Prob (F-statistic):           4.62e-21\n",
       "Time:                        18:00:23   Log-Likelihood:                -433.07\n",
       "No. Observations:                 101   AIC:                             872.1\n",
       "Df Residuals:                      98   BIC:                             880.0\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -33.8288     10.950     -3.089      0.003     -55.558     -12.100\n",
       "x1             1.0269      1.279      0.803      0.424      -1.512       3.566\n",
       "x2             4.9153      0.976      5.034      0.000       2.978       6.853\n",
       "==============================================================================\n",
       "Omnibus:                        0.676   Durbin-Watson:                   0.980\n",
       "Prob(Omnibus):                  0.713   Jarque-Bera (JB):                0.806\n",
       "Skew:                           0.163   Prob(JB):                        0.668\n",
       "Kurtosis:                       2.708   Cond. No.                         177.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = ozone.iloc[:,[1,2]] # select columns of index 1 and 2 (i.e. x1 and x2)\n",
    "X = sm.add_constant(X)\n",
    "Y = ozone['y']\n",
    "model = sm.OLS(Y,X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Questions : \n",
    "    - What is the equation of the model ?\n",
    "    - What is the result of the Student's test on x1 ? What does it mean ?\n",
    "    - What is the R^2 of this model ? Compare it with the model where x2 is alone to predict y. Conclusion\n",
    "    - What is I_r for this model ? Compare it with the model where x2 is alone to predict y. Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y= -33.8288 + 1.0269*x1 +4.9153*x2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x1, ne sert pas en présence des autres variables car P>|t| >> 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6154227247464323"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rsquared"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0.6128945664852581 seul\n",
    "0.6154227247464323 avec x1\n",
    "donc x1 aide a apporter un tout petit peu plus dinformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31349.224015744603"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_r = np.sum((ozone['y'] - model.fittedvalues)**2)\n",
    "I_r"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#235.5648942306331 X2 alone  #on a pas calculé lerreur quadratique moyenne sur le x2 seul\n",
    "310.3883565915307 with x1, donc lerreur a diminué"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Use this model to predict the 10 new individuals (of the ozone_new dataset) and compute the mean square prediction error. Conclusion ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     92.607713\n",
       "1     49.053373\n",
       "2     92.776136\n",
       "3     88.961642\n",
       "4    104.022411\n",
       "5     74.912713\n",
       "6     73.188930\n",
       "7    109.671545\n",
       "8    154.626563\n",
       "9     83.635613\n",
       "dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then, we will prepare the new data so that it is under the same form as the data used to create the model,\n",
    "# i.e. one constant column and one column with x1 values for the new data\n",
    "X_new = ozone_new.iloc[:,[1,2]] # only the x1 column of the new dataset\n",
    "X_new = sm.add_constant(X_new) # add the constant column\n",
    "X_new\n",
    "# and then we can easily predict y for this new dataset:\n",
    "model.predict(X_new)\n",
    "# You can see the predictions for the 10 individuals of ozone_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225.94603156757847"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_error = np.mean((ozone_new['y'] - model.predict(X_new))**2)\n",
    "mean_error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You should conclude that the model with x1 and x2 is worse than x2 alone (to predict these new individuals), but the R^2 and I_r said that x1 and x2 is better than x2 alone.\n",
    "This is the problem I explained in the CM : R^2 and I_r are always in favor of models with more variables (even if you add variables completely independant of the target variable) and they do not ensure that a model will be good to predict new indiviuals.\n",
    "That's why a better criterion is the estimation of the generalization error. \n",
    "In the next exercice, we will apply the train / test split strategy to estimate this generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 : Estimation of the generalization error by train / test split."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before going with the train/test split, I advice you to create two functions that might help you with easily creating regression models and computing predictions.\n",
    "1) Define a function 'my_regression(data, idx_p, idx_t)'. The aim of this function is to create, using a dataset 'data', a regression model to predict the variable whose column index is 'idx_t' (index target) using the variables whose column indexes are in 'idx_p' (index predictive). \n",
    "For instance, the call my_regression(ozone, [2], 0)' should create the model to predict y (column 0 of ozone) using x2 (column 2 of ozone).\n",
    "And the call my_regression(ozone, [1,2], 0) should create the model to predict y (column 0 of ozone) using x1 and x2 (columns 1 and 2 of ozone).\n",
    "You have already done this kind of things you just have to wrap everything up in a function. The idea of this function is to have a simple ang generic way to create different models, without having to create the X, add a constant, create the Y and call the OLS each time you need to fit a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_regression(data, idx_p, idx_t):\n",
    "    X = data.iloc[:,idx_p] \n",
    "    X = sm.add_constant(X) \n",
    "    Y = data.iloc[:,idx_t]\n",
    "    mod = sm.OLS(Y, X).fit()\n",
    "    return(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   78.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 12 Apr 2021</td> <th>  Prob (F-statistic):</th> <td>4.62e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:38:24</td>     <th>  Log-Likelihood:    </th> <td> -433.07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   101</td>      <th>  AIC:               </th> <td>   872.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   880.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  -33.8288</td> <td>   10.950</td> <td>   -3.089</td> <td> 0.003</td> <td>  -55.558</td> <td>  -12.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.0269</td> <td>    1.279</td> <td>    0.803</td> <td> 0.424</td> <td>   -1.512</td> <td>    3.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    4.9153</td> <td>    0.976</td> <td>    5.034</td> <td> 0.000</td> <td>    2.978</td> <td>    6.853</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.676</td> <th>  Durbin-Watson:     </th> <td>   0.980</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.713</td> <th>  Jarque-Bera (JB):  </th> <td>   0.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.163</td> <th>  Prob(JB):          </th> <td>   0.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.708</td> <th>  Cond. No.          </th> <td>    177.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.615\n",
       "Model:                            OLS   Adj. R-squared:                  0.608\n",
       "Method:                 Least Squares   F-statistic:                     78.41\n",
       "Date:                Mon, 12 Apr 2021   Prob (F-statistic):           4.62e-21\n",
       "Time:                        17:38:24   Log-Likelihood:                -433.07\n",
       "No. Observations:                 101   AIC:                             872.1\n",
       "Df Residuals:                      98   BIC:                             880.0\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -33.8288     10.950     -3.089      0.003     -55.558     -12.100\n",
       "x1             1.0269      1.279      0.803      0.424      -1.512       3.566\n",
       "x2             4.9153      0.976      5.034      0.000       2.978       6.853\n",
       "==============================================================================\n",
       "Omnibus:                        0.676   Durbin-Watson:                   0.980\n",
       "Prob(Omnibus):                  0.713   Jarque-Bera (JB):                0.806\n",
       "Skew:                           0.163   Prob(JB):                        0.668\n",
       "Kurtosis:                       2.708   Cond. No.                         177.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = my_regression(ozone, [1,2], 0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2) Create a function 'my_prediction(my_model, data)' that returns the predictions made by the model 'my_model'\n",
    "for the individuals of the dataset 'data'. \n",
    "Be careful, you need to select the columns of the dataset 'data' that the model 'my_model' needs (i.e. the ones that were used to create the model).\n",
    "For that, you can find the names of the predictive variables of 'my_model' by the command my_model.model.exog_names, and then you have to select the columns having these names (by a .loc on data)\n",
    "Don't forget to add a constant to the data before the predictions (as in the exemple a few cells above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x1', 'x2']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co = model.model.exog_names\n",
    "co[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.3</td>\n",
       "      <td>21.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.7</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.9</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.1</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.8</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.3</td>\n",
       "      <td>18.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17.1</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.6</td>\n",
       "      <td>25.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27.0</td>\n",
       "      <td>32.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.7</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1    x2\n",
       "0  18.3  21.9\n",
       "1  13.7  14.0\n",
       "2  19.9  21.6\n",
       "3  18.1  21.2\n",
       "4  20.8  23.7\n",
       "5  18.3  18.3\n",
       "6  17.1  18.2\n",
       "7  19.6  25.1\n",
       "8  27.0  32.7\n",
       "9  17.7  20.2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch = ozone_new[co[1:]]\n",
    "fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_prediction(my_model,data):\n",
    "    \n",
    "    columns =  my_model.model.exog_names\n",
    "\n",
    "    X_new = data[columns[1:]] # all x column of the new dataset\n",
    "    X_new = sm.add_constant(X_new) # add the constant column\n",
    "    X_new\n",
    "    # and then we can easily predict y for this new dataset:\n",
    "    # You can see the predictions for the 10 individuals of ozone_new\n",
    "    predictions = my_model.predict(X_new)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3) Try to use your functions to create a regression model using ozone (with predictive variables of your choice) and then use this model to predict the individuals of ozone_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     92.480354\n",
       "1     58.705224\n",
       "2     85.862897\n",
       "3     84.020220\n",
       "4    108.625571\n",
       "5     70.660818\n",
       "6     70.200149\n",
       "7    112.457213\n",
       "8    155.321235\n",
       "9     84.648980\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =my_regression(ozone,[2,4],0)\n",
    "predictions = my_prediction(model,ozone_new)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4) Now we will see how to do a train / test split to estimate the generalization error of a regression model.\n",
    "First we will combine our two datasets (ozone and ozone_new in a single one). In a real prediction setting, you'll get one complete dataset (with predictive variables and a target one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>15.6</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>-0.6946</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17.7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>15.3</td>\n",
       "      <td>17.6</td>\n",
       "      <td>19.5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>1.8794</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114</td>\n",
       "      <td>16.2</td>\n",
       "      <td>19.7</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>-0.1736</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94</td>\n",
       "      <td>17.4</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-2.9544</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>59</td>\n",
       "      <td>18.3</td>\n",
       "      <td>18.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>70</td>\n",
       "      <td>17.1</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-7.8785</td>\n",
       "      <td>-5.1962</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>81</td>\n",
       "      <td>19.6</td>\n",
       "      <td>25.1</td>\n",
       "      <td>27.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>-2.5712</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>6.5778</td>\n",
       "      <td>4.3301</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>78</td>\n",
       "      <td>17.7</td>\n",
       "      <td>20.2</td>\n",
       "      <td>21.5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10\n",
       "0     87  15.6  18.5  18.4   4   4   8  0.6946 -1.7101 -0.6946   84\n",
       "1     82  17.0  18.4  17.7   5   5   7 -4.3301 -4.0000 -3.0000   87\n",
       "2     92  15.3  17.6  19.5   2   5   4  2.9544  1.8794  0.5209   82\n",
       "3    114  16.2  19.7  22.5   1   1   0  0.9848  0.3473 -0.1736   92\n",
       "4     94  17.4  20.5  20.4   8   8   7 -0.5000 -2.9544 -4.3301  114\n",
       "..   ...   ...   ...   ...  ..  ..  ..     ...     ...     ...  ...\n",
       "106   59  18.3  18.3  19.0   7   7   7 -3.9392 -1.9284 -1.7101   66\n",
       "107   70  17.1  18.2  18.0   7   7   7 -4.3301 -7.8785 -5.1962   72\n",
       "108   81  19.6  25.1  27.2   3   4   4 -1.9284 -2.5712 -4.3301   57\n",
       "109  146  27.0  32.7  33.7   0   0   0  2.9544  6.5778  4.3301  121\n",
       "110   78  17.7  20.2  21.5   5   5   3  0.0000  0.5209  0.0000   59\n",
       "\n",
       "[111 rows x 11 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ozoneAll = pd.concat([ozone,ozone_new], ignore_index=True)\n",
    "ozoneAll"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We now need to split ozone into two datasets : train and test with about 75% of ozone into train and 25% into test (randomly). There is a function that can do this automatically in the package sklearn : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train , test = train_test_split(ozoneAll, test_size = 0.25) \n",
    "# just need to give the input dataset (ozone here) and the fraction that you want in the test set (25% here)\n",
    "# the two created datasets are called train and test as we have asked"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Check that the dimensions of train and test are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 11)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 11)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To be able to compare the results that you will obtain with the ones that I have obtained, we need to have the \n",
    "same train / test split. For this, you can set a parameter 'random_state = 20' in the train_test_split function (I have also used random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train , test = train_test_split(ozoneAll, test_size = 0.25, random_state = 20) \n",
    "# now we will use these train and test sets until the end of this TP (unless specified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>78</td>\n",
       "      <td>17.7</td>\n",
       "      <td>20.2</td>\n",
       "      <td>21.5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>108</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.5</td>\n",
       "      <td>25.1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.5712</td>\n",
       "      <td>-3.8567</td>\n",
       "      <td>-4.6985</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>117</td>\n",
       "      <td>21.6</td>\n",
       "      <td>26.9</td>\n",
       "      <td>28.6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5321</td>\n",
       "      <td>1.9284</td>\n",
       "      <td>1.9284</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>156</td>\n",
       "      <td>24.9</td>\n",
       "      <td>30.5</td>\n",
       "      <td>32.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-1.8794</td>\n",
       "      <td>-1.2856</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>109</td>\n",
       "      <td>20.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>26.6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0261</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>-3.2139</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>113</td>\n",
       "      <td>17.5</td>\n",
       "      <td>18.2</td>\n",
       "      <td>22.7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.7588</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>-4.6985</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>76</td>\n",
       "      <td>13.3</td>\n",
       "      <td>17.7</td>\n",
       "      <td>17.7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.9397</td>\n",
       "      <td>-0.7660</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>81</td>\n",
       "      <td>16.2</td>\n",
       "      <td>22.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>-2.5712</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>42</td>\n",
       "      <td>12.7</td>\n",
       "      <td>14.3</td>\n",
       "      <td>14.9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-2.5000</td>\n",
       "      <td>-3.2139</td>\n",
       "      <td>-2.5000</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>83</td>\n",
       "      <td>16.9</td>\n",
       "      <td>19.8</td>\n",
       "      <td>22.1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>-3.7588</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10\n",
       "110   78  17.7  20.2  21.5   5   5   3  0.0000  0.5209  0.0000   59\n",
       "19   108  24.0  23.5  25.1   4   4   0 -2.5712 -3.8567 -4.6985  146\n",
       "69   117  21.6  26.9  28.6   6   6   4  1.5321  1.9284  1.9284  113\n",
       "50   156  24.9  30.5  32.2   0   1   4 -0.5000 -1.8794 -1.2856  160\n",
       "65   109  20.8  23.7  26.6   8   5   4 -1.0261 -1.7101 -3.2139  116\n",
       "..   ...   ...   ...   ...  ..  ..  ..     ...     ...     ...  ...\n",
       "28   113  17.5  18.2  22.7   8   8   5 -3.7588 -3.9392 -4.6985   97\n",
       "95    76  13.3  17.7  17.7   7   7   6 -0.9397 -0.7660 -0.5000   92\n",
       "15    81  16.2  22.4  23.4   8   3   1  0.0000  0.3473 -2.5712  145\n",
       "90    42  12.7  14.3  14.9   8   7   7 -2.5000 -3.2139 -2.5000   60\n",
       "99    83  16.9  19.8  22.1   6   5   3 -4.0000 -3.7588 -4.0000   99\n",
       "\n",
       "[83 rows x 11 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>131</td>\n",
       "      <td>22.7</td>\n",
       "      <td>28.4</td>\n",
       "      <td>30.1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1736</td>\n",
       "      <td>-1.9696</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17.7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>153</td>\n",
       "      <td>23.8</td>\n",
       "      <td>27.7</td>\n",
       "      <td>29.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9397</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80</td>\n",
       "      <td>17.7</td>\n",
       "      <td>19.8</td>\n",
       "      <td>18.3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>-5.6382</td>\n",
       "      <td>-5.0000</td>\n",
       "      <td>-6.0000</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>149</td>\n",
       "      <td>23.3</td>\n",
       "      <td>27.6</td>\n",
       "      <td>28.8</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>-1.5321</td>\n",
       "      <td>-0.1736</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>71</td>\n",
       "      <td>19.2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>-7.8785</td>\n",
       "      <td>-6.8937</td>\n",
       "      <td>-6.8937</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>77</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>23.6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>-3.4641</td>\n",
       "      <td>-2.5981</td>\n",
       "      <td>-3.7588</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>15.3</td>\n",
       "      <td>17.6</td>\n",
       "      <td>19.5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>1.8794</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>54</td>\n",
       "      <td>18.6</td>\n",
       "      <td>18.7</td>\n",
       "      <td>17.8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-4.6985</td>\n",
       "      <td>-2.5000</td>\n",
       "      <td>-0.8682</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>101</td>\n",
       "      <td>16.9</td>\n",
       "      <td>17.8</td>\n",
       "      <td>20.6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-2.0000</td>\n",
       "      <td>-0.5209</td>\n",
       "      <td>1.8794</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>74</td>\n",
       "      <td>15.8</td>\n",
       "      <td>18.7</td>\n",
       "      <td>19.1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.5963</td>\n",
       "      <td>-6.8937</td>\n",
       "      <td>-7.5175</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>159</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.3</td>\n",
       "      <td>26.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.3420</td>\n",
       "      <td>1.2856</td>\n",
       "      <td>-2.0000</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>76</td>\n",
       "      <td>17.8</td>\n",
       "      <td>21.3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.0642</td>\n",
       "      <td>-2.2981</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>67</td>\n",
       "      <td>16.9</td>\n",
       "      <td>19.1</td>\n",
       "      <td>19.5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.2981</td>\n",
       "      <td>-3.7588</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>71</td>\n",
       "      <td>15.4</td>\n",
       "      <td>17.7</td>\n",
       "      <td>16.6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.8302</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3892</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>98</td>\n",
       "      <td>15.2</td>\n",
       "      <td>19.7</td>\n",
       "      <td>20.3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>4.3301</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>76</td>\n",
       "      <td>17.3</td>\n",
       "      <td>22.7</td>\n",
       "      <td>24.6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.9544</td>\n",
       "      <td>-2.9544</td>\n",
       "      <td>-2.0000</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>70</td>\n",
       "      <td>18.8</td>\n",
       "      <td>22.7</td>\n",
       "      <td>24.9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6840</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3681</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>84</td>\n",
       "      <td>17.4</td>\n",
       "      <td>20.4</td>\n",
       "      <td>21.4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>-2.5981</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>77</td>\n",
       "      <td>16.2</td>\n",
       "      <td>20.8</td>\n",
       "      <td>22.1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.6946</td>\n",
       "      <td>-2.0000</td>\n",
       "      <td>-1.3681</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>83</td>\n",
       "      <td>15.4</td>\n",
       "      <td>17.4</td>\n",
       "      <td>16.6</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-2.0521</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>15.6</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>-0.6946</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>59</td>\n",
       "      <td>18.3</td>\n",
       "      <td>18.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>72</td>\n",
       "      <td>18.1</td>\n",
       "      <td>21.2</td>\n",
       "      <td>23.9</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.5981</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>-3.7588</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>83</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>24.1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.0261</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>92</td>\n",
       "      <td>16.7</td>\n",
       "      <td>19.1</td>\n",
       "      <td>19.3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.0521</td>\n",
       "      <td>-4.4995</td>\n",
       "      <td>-2.7362</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>63</td>\n",
       "      <td>15.1</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>-5.3623</td>\n",
       "      <td>-6.1284</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>69</td>\n",
       "      <td>15.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15.9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-4.5963</td>\n",
       "      <td>-3.8302</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10\n",
       "70   131  22.7  28.4  30.1   5   3   3  0.1736 -1.9696 -1.9284  117\n",
       "1     82  17.0  18.4  17.7   5   5   7 -4.3301 -4.0000 -3.0000   87\n",
       "46   153  23.8  27.7  29.4   1   1   4  0.9397  1.5000  0.0000  149\n",
       "5     80  17.7  19.8  18.3   6   6   7 -5.6382 -5.0000 -6.0000   94\n",
       "48   149  23.3  27.6  28.8   4   6   3  0.8660 -1.5321 -0.1736  159\n",
       "31    71  19.2  21.0  22.4   6   4   6 -7.8785 -6.8937 -6.8937   77\n",
       "54    77  20.0  18.2  23.6   5   7   6 -3.4641 -2.5981 -3.7588  116\n",
       "2     92  15.3  17.6  19.5   2   5   4  2.9544  1.8794  0.5209   82\n",
       "56    54  18.6  18.7  17.8   8   8   8 -4.6985 -2.5000 -0.8682   63\n",
       "76   101  16.9  17.8  20.6   7   7   7 -2.0000 -0.5209  1.8794  112\n",
       "86    74  15.8  18.7  19.1   8   7   7 -4.5963 -6.8937 -7.5175   78\n",
       "47   159  24.0  28.3  26.5   2   2   7 -0.3420  1.2856 -2.0000  153\n",
       "67    76  17.8  21.3  24.0   7   5   5 -3.0642 -2.2981 -3.9392   67\n",
       "35    67  16.9  19.1  19.5   5   5   6 -2.2981 -3.7588  0.0000   67\n",
       "89    71  15.4  17.7  16.6   4   5   5 -3.8302  0.0000  1.3892   69\n",
       "93    98  15.2  19.7  20.3   2   2   2  4.0000  5.0000  4.3301   96\n",
       "79    76  17.3  22.7  24.6   4   5   6 -2.9544 -2.9544 -2.0000   78\n",
       "23    70  18.8  22.7  24.9   5   2   1  0.6840  0.0000  1.3681   67\n",
       "36    84  17.4  20.4  21.4   3   4   6  0.0000  0.3473 -2.5981   67\n",
       "97    77  16.2  20.8  22.1   6   5   5 -0.6946 -2.0000 -1.3681   71\n",
       "12    83  15.4  17.4  16.6   8   7   7 -4.3301 -2.0521 -3.0000   70\n",
       "0     87  15.6  18.5  18.4   4   4   8  0.6946 -1.7101 -0.6946   84\n",
       "106   59  18.3  18.3  19.0   7   7   7 -3.9392 -1.9284 -1.7101   66\n",
       "104   72  18.1  21.2  23.9   7   6   4 -2.5981 -3.9392 -3.7588  113\n",
       "44    83  19.0  22.5  24.1   2   4   6  0.0000 -1.0261  0.5209   81\n",
       "39    92  16.7  19.1  19.3   7   6   4 -2.0521 -4.4995 -2.7362   69\n",
       "37    63  15.1  20.5  20.6   8   6   6  2.0000 -5.3623 -6.1284   84\n",
       "38    69  15.1  15.6  15.9   8   8   8 -4.5963 -3.8302 -4.3301   63"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can now easily estimate the generalization error of a regression model using the procedure shown in the CM:\n",
    " - fit a regression model with the train data (with predictive variables that you want)\n",
    " - predict using this model the individuals of the test set\n",
    " - compute the mean squared error of these predictions : it is the estimation of the generalization error\n",
    "Question : write a function 'generalization_error_split(train, test, idx_p, idx_t)' that estimates the generaliztion error by a train / test split procedure (with the train and test sets given in parameters) for a regression model that uses the variables in the columns of index idx_p to predict the target variable of index idx_t (in the datasets)\n",
    "Hint : with the functions my_regression and my_predictions above, it is quite easy\n",
    "Apply it to estimate the generalization error of a regression model that predicts y using x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229.44106488635316"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =my_regression(train,[10],0)\n",
    "predictions = my_prediction(model,test)\n",
    "predictions\n",
    "np.mean((predictions-test.iloc[:,0])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229.44106488635316"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_r = np.mean((test['y'] - predictions)**2)\n",
    "I_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7fbb59e11640>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =my_regression(train,[1],0)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70     117.754739\n",
       "1       80.950972\n",
       "46     124.857220\n",
       "5       85.470732\n",
       "48     121.628819\n",
       "31      95.155934\n",
       "54     100.321375\n",
       "2       69.974409\n",
       "56      91.281854\n",
       "76      80.305291\n",
       "86      73.202810\n",
       "47     126.148580\n",
       "67      86.116413\n",
       "35      80.305291\n",
       "89      70.620090\n",
       "93      69.328729\n",
       "79      82.888012\n",
       "23      92.573214\n",
       "36      83.533692\n",
       "97      75.785531\n",
       "12      70.620090\n",
       "0       71.911450\n",
       "106     89.344813\n",
       "104     88.053453\n",
       "44      93.864574\n",
       "39      79.013931\n",
       "37      68.683049\n",
       "38      68.683049\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = my_prediction(model,test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349.8933173728155"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_r = np.mean((test.iloc[:,0] - predictions)**2)\n",
    "I_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalization_error_split(train, test, idx_p, idx_t):\n",
    "    #model =my_regression(train,[10],0)\n",
    "    #predictions = my_prediction(model,test)\n",
    "    #np.mean((predictions-test.iloc[:,0])**2)\n",
    "    \n",
    "    mod =my_regression(train,idx_p,idx_t)\n",
    "    pred = my_prediction(mod,test)\n",
    "    diff = pred-test.iloc[:,idx_t]\n",
    "    I_r = np.mean(diff**2)\n",
    "    \n",
    "    return(I_r)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def generalization_error_split(train, test, idx_p, idx_t):\n",
    "   \n",
    "    model = my_regression(train, idx_p, idx_t)\n",
    "    #my_prediction(model,test)\n",
    "   \n",
    "    diff = test.iloc[:,0] - my_prediction(model,test)\n",
    "    sqr = pow(diff,2)\n",
    "    print(f\"EQM est: {np.mean(sqr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306.78565783111577"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalization_error_split(train, test, 2, 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5) What is the best single variable to predict y ? What is the estimation of the generalization error of such a model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349.8933173728155\n",
      "306.78565783111577\n",
      "408.0046826177133\n",
      "497.4068155876456\n",
      "537.3194762458469\n",
      "673.3548910931984\n",
      "610.7579079141307\n",
      "585.9510471825889\n",
      "713.1390398579188\n",
      "229.44106488635316\n"
     ]
    }
   ],
   "source": [
    "for j in np.arange (1,ozoneAll.shape[1]):\n",
    "    print(generalization_error_split(train, test, [j], 0))\n",
    "    #print(j)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The best one is the X10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Excepeted result from Professor (Simon Malinowski)\n",
    "1\n",
    "349.8933173728155\n",
    "2\n",
    "306.78565783111577\n",
    "3\n",
    "408.0046826177133\n",
    "4\n",
    "497.4068155876456\n",
    "5\n",
    "537.3194762458469\n",
    "6\n",
    "673.3548910931984\n",
    "7\n",
    "610.7579079141307\n",
    "8\n",
    "585.9510471825889\n",
    "9\n",
    "713.1390398579188\n",
    "10\n",
    "229.44106488635316"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6) Is the model using variables x2 and x3 better than the one using x2 only ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361.9297843156946"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error2_3 =generalization_error_split(train, test, [2,3], 0)\n",
    "error2_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306.78565783111577"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error2 = generalization_error_split(train, test, [2], 0)\n",
    "error2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NO, single X2 is better"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7) Is the model using all the variables (x1 to x10) better than the one using x2 only ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224.18479524388653"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalization_error_split(train, test, [1,2,3,4,5,6,7,8,9,10], 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is better , but it can be wrong due to number of variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8) Is the model using variables x2, x4, x8, x10 better than the one using all the variables ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.04317778126762"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalization_error_split(train, test, [2,4,8,10], 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The train/test split procedure is one way to estimate the generalization error of a regression model. One of its main drawback is that it is not very stable as it is really dependent on the random split. Two different splits might give very different estimations of the generalization error.\n",
    "A more stable way to estimate the generalization error is the K-fold cross validation procedure as explained in the CM. You will implement this a bit later during next TP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
